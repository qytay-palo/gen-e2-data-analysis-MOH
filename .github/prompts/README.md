# Data Science Lifecycle Automation Prompts

This directory contains a comprehensive prompt system for automating end-to-end data science and analytics projects.

## Overview

The prompt system consists of:
1. **Project Initialization Prompts** (0-2): Set up projects and generate user stories
2. **Planning Prompts** (3-4): Generate lifecycle stages and execution plans
3. **Orchestration Prompts** (5): Execute plans by routing to stage-specific prompts
4. **Stage-Specific Prompts** (`stages/`): Detailed instructions for each lifecycle stage

## Prompt Files

### Main Orchestration Prompts

| File | Purpose | When to Use |
|------|---------|-------------|
| `0-start-gen-e2-data-analysis-project.prompt.md` | Initialize new data analysis project | Starting a brand new project |
| `2-generate-user-stories-from-data-flow.prompt.md` | Generate user stories from objectives | After defining project goals |
| `4-generate-lifecycle-and-execution-plan.prompt.md` | Map user stories to lifecycle stages and create execution plan | After user stories are defined |
| `5-orchestrate-execution.prompt.md` | Execute the project according to the plan | Ready to start implementation |

### Stage-Specific Prompts (`stages/`)

Each lifecycle stage has a dedicated prompt with detailed instructions:

| Stage | Prompt File | Inputs | Outputs |
|-------|------------|--------|---------|
| **Data Acquisition** | `data-acquisition.prompt.md` | Connection details, queries, target directory | Raw datasets, extraction scripts, metadata |
| **Data Quality Assessment** | `data-quality-check.prompt.md` | Dataset path, validation rules | Quality reports, issue logs, profiling stats |
| **Exploratory Data Analysis** | `eda.prompt.md` | Clean dataset, analysis objectives | EDA notebooks, visualizations, insights |
| **Data Preparation** | `data-preparation.prompt.md` | Raw/interim data, transformation specs | Cleaned datasets, transformation logs |
| **Feature Engineering** | `feature-engineering.prompt.md` | Prepared data, feature specs | Feature datasets, feature documentation |
| **Model Training** | `model-training.prompt.md` | Training data, model config, metrics | Trained models, training logs, metrics |
| **Statistical Analysis** | `statistical-analysis.prompt.md` | Analysis data, hypotheses | Test results, confidence intervals |
| **Visualization & Reporting** | `visualization-reporting.prompt.md` | Results data, audience type | Visualizations, reports, dashboards |

## Usage Workflow

### Quick Start

```bash
# 1. Initialize project
Use prompt: 0-start-gen-e2-data-analysis-project.prompt.md

# 2. Generate user stories
Use prompt: 2-generate-user-stories-from-data-flow.prompt.md

# 3. Generate execution plan
Use prompt: 4-generate-lifecycle-and-execution-plan.prompt.md

# 4. Execute the project
Use prompt: 5-orchestrate-execution.prompt.md
```

### Detailed Workflow

#### Phase 1: Project Setup
1. **Prompt**: `0-start-gen-e2-data-analysis-project.prompt.md`
2. **Action**: Answer questions about objectives, data sources, stakeholders, tools
3. **Output**: 
   - Updated project structure
   - Documentation in `docs/`
   - Configuration files in `config/`

#### Phase 2: User Story Generation
1. **Prompt**: `2-generate-user-stories-from-data-flow.prompt.md`
2. **Inputs**: Read from `docs/objectives/opportunities.md` and `docs/project_context/`
3. **Output**: User stories in `docs/objectives/user_stories/`

#### Phase 3: Execution Planning
1. **Prompt**: `4-generate-lifecycle-and-execution-plan.prompt.md`
2. **Inputs**: Read all user stories
3. **Actions**:
   - Analyze each user story
   - Map to lifecycle stages
   - Create execution sequence with waves
   - Design prompt routing strategy
4. **Outputs**:
   - `docs/project_context/execution_plan.yml`
   - `config/prompt_routing.yml`
   - `scripts/execute_lifecycle.py` (pseudocode)

#### Phase 4: Execution
1. **Prompt**: `5-orchestrate-execution.prompt.md`
2. **Process**:
   - Load execution plan
   - For each wave:
     - For each user story:
       - For each lifecycle stage:
         - Call appropriate stage prompt
         - Pass inputs from context
         - Validate outputs
         - Update context
3. **Outputs**: Artifacts as specified in execution plan

## Execution Plan Structure

The execution plan (generated by prompt 4) has this structure:

```yaml
execution_plan:
  project_name: "Your Project Name"
  generated_date: "2026-01-28"
  
  waves:
    - wave_id: 1
      name: "Foundation & Data Quality"
      duration: "2 weeks"
      user_stories:
        - story_id: "01"
          title: "Establish Data Quality Baseline"
          lifecycle_stages:
            - stage: "Data Acquisition & Access"
              tasks: [...]
              estimated_effort: "1 day"
            - stage: "Data Quality Assessment"
              tasks: [...]
              estimated_effort: "2 days"
          dependencies: []
          priority: "High"
```

## Prompt Routing Configuration

The routing config (generated by prompt 4) maps stages to prompts:

```yaml
prompt_routing:
  data_acquisition:
    prompt_file: ".github/prompts/stages/data-acquisition.prompt.md"
    inputs_required:
      - data_source_connection_string
      - tables_list
      - target_directory
    outputs_generated:
      - raw_data_files
      - data_extraction_log
  
  exploratory_data_analysis:
    prompt_file: ".github/prompts/stages/eda.prompt.md"
    inputs_required:
      - dataset_path
      - analysis_objectives
    outputs_generated:
      - eda_notebook
      - insights_summary
```

## Creating New Stage Prompts

To add a new lifecycle stage prompt:

1. **Create file**: `.github/prompts/stages/your-stage-name.prompt.md`

2. **Use this template**:
```markdown
# [Stage Name] Stage

## Purpose
[What this stage accomplishes]

## Your Role
[Persona and expertise needed]

## Inputs
- **input_1**: [Description]
- **input_2**: [Description]

## Process
### Step 1: [Step Name]
[Detailed instructions]

### Step 2: [Next Step]
[More instructions]

## Outputs
1. **output_1**: [Description and file path]
2. **output_2**: [Description and file path]

## Quality Checks
- [ ] Check 1
- [ ] Check 2

## Next Steps
[What to do after this stage completes]
```

3. **Update routing config**: Add entry in `config/prompt_routing.yml`

## Data Science Lifecycle Framework

The prompts are organized around these standard lifecycle stages:

1. **Business Understanding & Scoping**
2. **Data Acquisition & Access**
3. **Data Quality Assessment**
4. **Exploratory Data Analysis (EDA)**
5. **Data Preparation & Transformation**
6. **Feature Engineering**
7. **Model Development**
8. **Model Evaluation**
9. **Statistical Analysis**
10. **Visualization & Reporting**
11. **Model Deployment & Operationalization**
12. **Monitoring & Maintenance**

Each stage has specific:
- Inputs required
- Processes to execute
- Outputs generated
- Quality checks
- Next steps

## Advanced: Automation with LangGraph

For production automation, implement with LangGraph or similar:

```python
from langgraph.graph import StateGraph
from typing import TypedDict

class ProjectState(TypedDict):
    current_wave: int
    current_story: str
    current_stage: str
    artifacts: dict
    logs: list

# Define nodes for each stage
def data_acquisition_node(state):
    # Call data-acquisition.prompt.md
    result = execute_prompt("stages/data-acquisition.prompt.md", state["artifacts"])
    return {"artifacts": {**state["artifacts"], **result}}

def eda_node(state):
    # Call eda.prompt.md
    result = execute_prompt("stages/eda.prompt.md", state["artifacts"])
    return {"artifacts": {**state["artifacts"], **result}}

# Build graph
workflow = StateGraph(ProjectState)
workflow.add_node("data_acquisition", data_acquisition_node)
workflow.add_node("eda", eda_node)
workflow.add_edge("data_acquisition", "eda")
workflow.set_entry_point("data_acquisition")

# Execute
app = workflow.compile()
result = app.invoke(initial_state)
```

## Best Practices

### For Prompt Authors
- ✅ **Be specific**: Provide clear, actionable instructions
- ✅ **Include examples**: Show expected inputs/outputs
- ✅ **Define quality checks**: Specify validation criteria
- ✅ **Document dependencies**: Note what must happen before/after
- ✅ **Use consistent structure**: Follow the template format

### For Prompt Users
- ✅ **Follow the sequence**: Don't skip stages unless justified
- ✅ **Validate outputs**: Check quality criteria before proceeding
- ✅ **Track progress**: Use execution logs and checkpoints
- ✅ **Handle errors gracefully**: Use error recovery procedures
- ✅ **Document deviations**: Note any changes to the plan

### For Orchestrators
- ✅ **Pass context properly**: Ensure outputs become inputs for next stage
- ✅ **Log everything**: Maintain audit trail of execution
- ✅ **Checkpoint frequently**: Enable recovery from failures
- ✅ **Validate prerequisites**: Check dependencies before executing
- ✅ **Monitor progress**: Provide visibility to stakeholders

## Troubleshooting

### Common Issues

**Issue**: Stage fails due to missing input
- **Solution**: Check that previous stage produced expected output
- **Prevention**: Validate stage outputs before proceeding

**Issue**: Execution plan too complex
- **Solution**: Break into smaller waves or simplify user stories
- **Prevention**: Keep user stories focused and atomic

**Issue**: Prompt instructions ambiguous
- **Solution**: Refer to examples or request clarification
- **Prevention**: Improve prompt with more examples and detail

**Issue**: Outputs not in expected format
- **Solution**: Review stage prompt output specifications
- **Prevention**: Add validation logic to check output formats

## Contributing

To improve the prompt system:

1. **Enhance existing prompts**: Add examples, clarify instructions, fix errors
2. **Create new stage prompts**: Follow template and naming conventions
3. **Update documentation**: Keep this README in sync with changes
4. **Share learnings**: Document patterns and anti-patterns discovered

## Examples

### Example 1: Simple Descriptive Analysis Project

**User Stories**: 2-3 stories focused on data quality and descriptive statistics

**Lifecycle Stages**:
- Data Acquisition
- Data Quality Assessment
- Exploratory Data Analysis
- Statistical Analysis
- Visualization & Reporting

**Execution**: Single wave, sequential execution

### Example 2: Predictive Modeling Project

**User Stories**: 5-7 stories including baseline, model development, evaluation

**Lifecycle Stages**:
- Data Acquisition
- Data Quality Assessment
- Exploratory Data Analysis
- Data Preparation & Transformation
- Feature Engineering
- Model Development
- Model Evaluation
- Visualization & Reporting

**Execution**: Multiple waves with dependencies

### Example 3: End-to-End ML System

**User Stories**: 10+ stories including deployment and monitoring

**Lifecycle Stages**: All 12 stages

**Execution**: 4-5 waves with parallel execution where possible

## References

- **Data Science Lifecycle**: CRISP-DM, TDSP (Team Data Science Process)
- **Agile Analytics**: User stories, sprints, iterative development
- **MLOps**: Model deployment, monitoring, retraining pipelines
- **Orchestration**: LangGraph, Airflow, Prefect for workflow automation

## Support

For questions or issues:
1. Check this documentation
2. Review example outputs in `results/execution_logs/`
3. Consult stage-specific prompt files
4. Refer to project documentation in `docs/`

---

**Last Updated**: 2026-01-28  
**Version**: 1.0  
**Maintainer**: Data Science Team
